{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkALnbLNHeI7"
   },
   "source": [
    "# **Cloudphysician's The Vital Extraction Challenge** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfF5vs4rHkfo"
   },
   "source": [
    "## **Problem Statement:**\n",
    "Patient monitoring is crucial in healthcare, as it allows healthcare professionals to closely track a patient's vital signs and detect any potential issues before they become serious. In particular, monitoring a patient's vitals, such as heart rate, blood pressure, and oxygen levels, can provide valuable information about a patient's overall health and well-being. The core problem statement is to extract *Heart Rate, SpO2, RR, Systolic Blood Pressure, Diabolic Blood Pressure, and MAP* from images of ECG Machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-ZMOPZ7HnTN"
   },
   "source": [
    "## **Proposed Pipeline:**\n",
    "The proposed pipeline consists of a total of four stages. The whole pipeline is made such that it can work in two modes: **'Fast'** and **'Accurate**'. \n",
    "**By default, the pipeline is set to 'Accurate' mode**. \n",
    "\n",
    "The 'Fast' mode ensures that the whole pipeline runs under 2 seconds on CPU, while the 'Accurate’ mode ensures maximum accuracy at each stage, thus ensuring the best overall performance. \n",
    "\n",
    "\n",
    " The four stages of the pipeline are as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofDIXWxIHqNu"
   },
   "source": [
    "[![url](https://i.postimg.cc/k58kF1Bw/Untitled-Diagram-drawio-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFTEigZNHtGj"
   },
   "source": [
    "\n",
    "\n",
    "1. Screen Extraction\n",
    "2. Number Detection\n",
    "3. OCR\n",
    "4. Number Classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CknZtzg6wr6q"
   },
   "source": [
    "# Downloading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oziZrSNyHxT3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_qcfB4FUb7K"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install yolov5\n",
    "# !pip install paddlepaddle -qq\n",
    "# !pip install paddleocr -qq\n",
    "!pip install ensemble-boxes -qq\n",
    "!pip install timm\n",
    "!pip install torchvision\n",
    "!pip install pytorch_lightning\n",
    "!pip install imutils\n",
    "\n",
    "## Kindly Restart your Runtime after this cell, to execute them on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Using cached timm-0.6.13-py3-none-any.whl (549 kB)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from timm) (1.12.1)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: pyyaml in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: torchvision in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from timm) (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.7->timm) (4.1.1)\n",
      "Requirement already satisfied: requests in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->timm) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->timm) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->timm) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->timm) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2021.10.8)\n",
      "Requirement already satisfied: numpy in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from torchvision->timm) (9.0.1)\n",
      "Installing collected packages: huggingface-hub, timm\n",
      "Successfully installed huggingface-hub-0.13.4 timm-0.6.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ensemble_boxes\n",
      "  Using cached ensemble_boxes-1.0.9-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numba in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from ensemble_boxes) (0.55.1)\n",
      "Requirement already satisfied: numpy in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from ensemble_boxes) (1.24.1)\n",
      "Requirement already satisfied: pandas in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from ensemble_boxes) (1.4.2)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from numba->ensemble_boxes) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from numba->ensemble_boxes) (61.2.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp39-cp39-macosx_10_9_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pandas->ensemble_boxes) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pandas->ensemble_boxes) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->ensemble_boxes) (1.16.0)\n",
      "Installing collected packages: numpy, ensemble-boxes\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\u001b[0m\n",
      "Successfully installed ensemble-boxes-1.0.9 numpy-1.21.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ensemble_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paddleocr in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (2.6.1.3)\n",
      "Requirement already satisfied: cython in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (0.29.28)\n",
      "Requirement already satisfied: lmdb in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (1.4.1)\n",
      "Requirement already satisfied: scikit-image in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (0.19.2)\n",
      "Requirement already satisfied: rapidfuzz in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (2.15.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (4.11.1)\n",
      "Requirement already satisfied: numpy in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (1.21.6)\n",
      "Requirement already satisfied: PyMuPDF<1.21.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (1.20.2)\n",
      "Requirement already satisfied: pdf2docx in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (0.5.6)\n",
      "Requirement already satisfied: visualdl in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (2.5.1)\n",
      "Requirement already satisfied: premailer in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (3.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (4.64.0)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (4.25.0)\n",
      "Requirement already satisfied: attrdict in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (2.0.1)\n",
      "Requirement already satisfied: python-docx in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (0.8.11)\n",
      "Requirement already satisfied: opencv-contrib-python<=4.6.0.66 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (4.6.0.66)\n",
      "Requirement already satisfied: opencv-python<=4.6.0.66 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (4.6.0.66)\n",
      "Requirement already satisfied: shapely in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (2.0.0)\n",
      "Requirement already satisfied: lxml in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (4.8.0)\n",
      "Requirement already satisfied: openpyxl in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (3.0.9)\n",
      "Requirement already satisfied: fire>=0.3.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (0.5.0)\n",
      "Requirement already satisfied: imgaug in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (0.4.0)\n",
      "Requirement already satisfied: pyclipper in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from paddleocr) (1.3.0.post4)\n",
      "Requirement already satisfied: six in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fire>=0.3.0->paddleocr) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fire>=0.3.0->paddleocr) (1.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->paddleocr) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from imgaug->paddleocr) (3.5.1)\n",
      "Requirement already satisfied: Pillow in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from imgaug->paddleocr) (9.0.1)\n",
      "Requirement already satisfied: scipy in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from imgaug->paddleocr) (1.10.0)\n",
      "Requirement already satisfied: imageio in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from imgaug->paddleocr) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from scikit-image->paddleocr) (2.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from scikit-image->paddleocr) (21.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from scikit-image->paddleocr) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from scikit-image->paddleocr) (2021.7.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->scikit-image->paddleocr) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->imgaug->paddleocr) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->imgaug->paddleocr) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->imgaug->paddleocr) (2.8.2)\n",
      "Requirement already satisfied: et-xmlfile in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from openpyxl->paddleocr) (1.1.0)\n",
      "Requirement already satisfied: cssutils in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from premailer->paddleocr) (2.6.0)\n",
      "Requirement already satisfied: requests in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from premailer->paddleocr) (2.27.1)\n",
      "Requirement already satisfied: cachetools in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from premailer->paddleocr) (4.2.2)\n",
      "Requirement already satisfied: cssselect in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from premailer->paddleocr) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->premailer->paddleocr) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->premailer->paddleocr) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->premailer->paddleocr) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->premailer->paddleocr) (2.10)\n",
      "Requirement already satisfied: rarfile in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (4.0)\n",
      "Requirement already satisfied: pandas in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (1.4.2)\n",
      "Requirement already satisfied: x2paddle in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (1.4.1)\n",
      "Requirement already satisfied: bce-python-sdk in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (0.8.83)\n",
      "Requirement already satisfied: tritonclient[all] in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (2.32.0)\n",
      "Requirement already satisfied: onnx>=1.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (1.13.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (5.8.0)\n",
      "Requirement already satisfied: flask>=1.1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (2.2.3)\n",
      "Requirement already satisfied: Flask-Babel>=3.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (3.1.0)\n",
      "Requirement already satisfied: gradio in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from visualdl->paddleocr) (3.25.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from flask>=1.1.1->visualdl->paddleocr) (4.11.3)\n",
      "Requirement already satisfied: click>=8.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from flask>=1.1.1->visualdl->paddleocr) (8.0.4)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from flask>=1.1.1->visualdl->paddleocr) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from flask>=1.1.1->visualdl->paddleocr) (3.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from flask>=1.1.1->visualdl->paddleocr) (2.2.3)\n",
      "Requirement already satisfied: Babel>=2.12 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr) (2.12.1)\n",
      "Requirement already satisfied: pytz>=2022.7 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask>=1.1.1->visualdl->paddleocr) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->paddleocr) (2.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from onnx>=1.6.0->visualdl->paddleocr) (4.5.0)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from bce-python-sdk->visualdl->paddleocr) (3.17)\n",
      "Requirement already satisfied: future>=0.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from bce-python-sdk->visualdl->paddleocr) (0.18.2)\n",
      "Requirement already satisfied: pydantic in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (1.10.7)\n",
      "Requirement already satisfied: pyyaml in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (6.0)\n",
      "Requirement already satisfied: semantic-version in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (2.10.0)\n",
      "Requirement already satisfied: aiofiles in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (23.1.0)\n",
      "Requirement already satisfied: gradio-client>=0.0.8 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.1.0)\n",
      "Requirement already satisfied: aiohttp in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (3.8.1)\n",
      "Requirement already satisfied: orjson in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (3.8.10)\n",
      "Requirement already satisfied: uvicorn in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.21.1)\n",
      "Requirement already satisfied: fastapi in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.95.1)\n",
      "Requirement already satisfied: ffmpy in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.3.0)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.3.3)\n",
      "Requirement already satisfied: python-multipart in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.0.6)\n",
      "Requirement already satisfied: websockets>=10.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (11.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.13.4)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (2.2.0)\n",
      "Requirement already satisfied: pydub in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.25.1)\n",
      "Requirement already satisfied: httpx in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (0.24.0)\n",
      "Requirement already satisfied: altair>=4.2.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio->visualdl->paddleocr) (4.2.2)\n",
      "Requirement already satisfied: entrypoints in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from altair>=4.2.0->gradio->visualdl->paddleocr) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from altair>=4.2.0->gradio->visualdl->paddleocr) (4.4.0)\n",
      "Requirement already satisfied: toolz in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from altair>=4.2.0->gradio->visualdl->paddleocr) (0.11.2)\n",
      "Requirement already satisfied: fsspec in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gradio-client>=0.0.8->gradio->visualdl->paddleocr) (2022.2.0)\n",
      "Requirement already satisfied: filelock in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13.0->gradio->visualdl->paddleocr) (3.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->visualdl->paddleocr) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->visualdl->paddleocr) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio->visualdl->paddleocr) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio->visualdl->paddleocr) (2.0.0)\n",
      "Requirement already satisfied: uc-micro-py in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->visualdl->paddleocr) (1.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio->visualdl->paddleocr) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio->visualdl->paddleocr) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio->visualdl->paddleocr) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio->visualdl->paddleocr) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio->visualdl->paddleocr) (4.0.1)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fastapi->gradio->visualdl->paddleocr) (0.26.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from starlette<0.27.0,>=0.26.1->fastapi->gradio->visualdl->paddleocr) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi->gradio->visualdl->paddleocr) (1.2.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from httpx->gradio->visualdl->paddleocr) (0.17.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->visualdl->paddleocr) (0.14.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tritonclient[all]->visualdl->paddleocr) (1.10)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tritonclient[all]->visualdl->paddleocr) (1.42.0)\n",
      "Requirement already satisfied: geventhttpclient<=2.0.2,>=1.4.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tritonclient[all]->visualdl->paddleocr) (2.0.2)\n",
      "Requirement already satisfied: gevent>=0.13 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr) (22.10.2)\n",
      "Requirement already satisfied: brotli in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr) (1.0.9)\n",
      "Requirement already satisfied: zope.event in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr) (4.6)\n",
      "Requirement already satisfied: zope.interface in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr) (5.4.0)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr) (2.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr) (61.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from x2paddle->visualdl->paddleocr) (1.10.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from sympy->x2paddle->visualdl->paddleocr) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install paddleocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36hxbgA6w4L6"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sHLD-7-Mw53i"
   },
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch -qq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "import imutils\n",
    "from skimage.transform import ProjectiveTransform\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import yolov5\n",
    "# import paddleocr\n",
    "# from paddleocr import PaddleOCR,draw_ocr\n",
    "from ensemble_boxes import *\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zl7iCnpoxVMT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1oFNh90rlv",
    "outputId": "b17320df-4651-4a12-ac98-fef0a0b71e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "99SslfSR1BNW"
   },
   "outputs": [],
   "source": [
    "# Change root to your location\n",
    "root = '/Users/aryanlath/Downloads/CloudPhysician-s-Vital-Extraction-Challenge-main/Saved Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwLy0LxxxRfk"
   },
   "source": [
    "# Screen Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs-N-nOMH9sP"
   },
   "source": [
    "\n",
    "The first stage of the pipeline is the extraction of monitor screens from the given images. We have approached the problem in two ways, which are as follows:\n",
    "-  #### **Corner Regression**: \n",
    "\n",
    "    The Corner Regression approach uses a CNN encoder to regress on the values of the bounding boxes of the desired screen, thereby producing an output vector of length eight (x1,y1,x2,y2,x3,y3,x4,y4 - coordinates of the bounding box). The followings are the details of the training parameters and augmentations for the CNN:\n",
    "\n",
    "   *  Model: **Fine-Tuned Resnet 34**\n",
    " * Augmentations: \n",
    "\n",
    "    We used a few custom augmentations in our pipeline because the standard libraries (e.g., Albumentations) do not provide augmentations on the random 4-sided polygon and are specifically tuned to rectangles. These custom augmentations improved the performance of our model and made it more robust.\n",
    "\n",
    "    **Custom Horizontal Flip**: Our custom augmentation performs a horizontal flip on the image and accordingly changes the bounding box coordinates, even in non-rectangular cases for the newly flipped image.\n",
    "\n",
    "    **Custom Random Crop**: Our custom implementation performs random crops to ensure that the full area of the screen is always visible in the augmented image. This random crop is implemented to mimic the scenarios where the model generally performs poorly (i.e., the original model performed poorly in images where the monitor was too close to the edge of the image). This augmentation significantly improved the model's performance on the edge cases, making it more robust to the actual scenarios.\n",
    "\n",
    "    **RandomBrightnessContrast**: RandomBrightnessContrast from the Albumentations library for more robust training on various input image scenarios. \n",
    "\n",
    "**Loss: Mean Squared Error**\n",
    "\n",
    "**Learning Rate: 0.001**\n",
    "\n",
    "**Epochs: 30**\n",
    "\n",
    "**Mean IoU: 0.873** \n",
    "\n",
    "* #### **UNet++ Semantic Segmentation**:\n",
    "\n",
    "UNet++ takes the original image as input and mask as ground truth. We generated the ground truth mask from the given bounding box coordinates using the cv2.fillPoly function. The details of the training, preprocessing, and post-processing are as follows:\n",
    "\n",
    "* Model: **UNET++ (from segmentation_models_pytorch)**\n",
    "* Encoder: **resnext101_32x8d**\n",
    "* Encoder weights: imagenet\n",
    "* Augmentations:\n",
    "    Except for the augmentations used in \"Corner Regression,\" we used one more augmentation for training purposes: ShiftScaleRotate: translate, scale, and rotate the input. \n",
    "* Preprocessing:\n",
    "    We use the same preprocessing as done on the original UNET++ pretraining. This normalizes the image to a distribution accustomed to the pre-trained weights.\n",
    "\n",
    "**Epochs: 15**\n",
    "\n",
    "**Learning Rate: 0.001**\n",
    "\n",
    "**Mean IoU: 0.896**\n",
    "\n",
    "**Loss: 0.75*Dice Loss + 0.25*BCE Loss**\n",
    "\n",
    "\n",
    "We made a custom weighted loss for the training of UNET++. Dice Loss increases the intersection with the ground truth image, while BCE Loss ensures confidence in predictions. This weighted average of losses resulted in better overall learning of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpJgPi4mIGNy"
   },
   "source": [
    "* #### **Area Union Fusion Ensembling**:\n",
    "    We used the predicted boxes from Corner Regression (I) and UNET++ (II) to ensemble for final prediction. We used Area Union Prediction method, i.e, the largest polygon that covers both the predicted polygons, thus ensuring maximum coverage.\n",
    "\n",
    "    ***Mean IoU Score: 0.91***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gPn-9FihxS9x"
   },
   "outputs": [],
   "source": [
    "## Defining Corner Regression Model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('resnet34', pretrained=True, num_classes=768)\n",
    "        self.ll = nn.Linear(768,8)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        x = self.model(img)\n",
    "        x = self.ll(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model_reg = CNN()\n",
    "\n",
    "\n",
    "## Define UNET++ Model\n",
    "\n",
    "ENCODER = 'resnext101_32x8d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "model_unet = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=1,\n",
    "    in_channels=3,\n",
    "    activation=None,\n",
    ")\n",
    "\n",
    "preprocessing_unet = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sN33pXpyks3",
    "outputId": "4d7bb115-b889-4532-bfd8-edeeda425d69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading weights\n",
    "\n",
    "model_reg = model_reg.to(device)\n",
    "model_unet = model_unet.to(device)\n",
    "\n",
    "model_reg.load_state_dict(torch.load(root + '/corner_reg.pt', map_location=device))\n",
    "model_unet.load_state_dict(torch.load(root + '/unet++_weights.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fCO6FmWA0HLw"
   },
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "        A.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return A.Compose(_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "      \n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_contour_from_mask(img):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img (np.array): mask for which bounding box has to be formed\n",
    "    Returns:\n",
    "        cnr (np.array): corners of the bounding box\n",
    "    \"\"\"\n",
    "\n",
    "    assert img.ndim == 2\n",
    "    h, w = img.shape[:2]\n",
    "    img = (img>0.9*img.max()) * 255\n",
    "    img = np.ascontiguousarray(img, dtype=np.uint8)\n",
    "\n",
    "    im_floodfill = img.copy()\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 255)\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    "    im_out = img | im_floodfill_inv\n",
    "    \n",
    "    im_open = cv2.morphologyEx(np.uint8(im_out), cv2.MORPH_OPEN, np.ones((5, 5)),iterations= 5)\n",
    "    image_sharp = cv2.morphologyEx(im_open, cv2.MORPH_CLOSE, np.ones((5, 5)),iterations= 5)\n",
    "\n",
    "    cnts = cv2.findContours(image_sharp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    peri = cv2.arcLength(c, True)\n",
    "    cnt = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "    \n",
    "    cnr = np.zeros((4,2))\n",
    "    cnr[0] = cnt[(cnt[:,:,0] + cnt[:,:,1]).argmin()][0]\n",
    "    cnr[1] = cnt[(cnt[:,:,0] - cnt[:,:,1]).argmax()][0]\n",
    "    cnr[2] = cnt[(cnt[:,:,0] + cnt[:,:,1]).argmax()][0]\n",
    "    cnr[3] = cnt[(cnt[:,:,1] - cnt[:,:,0]).argmax()][0]\n",
    "    \n",
    "    return cnr\n",
    "\n",
    "def corner_regression(img_path, model, size = 224):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_path (str): path to the image\n",
    "        model (torch.nn.Module): model for corner regression\n",
    "        size (int): size of the image to be fed to the model\n",
    "    Returns:\n",
    "        corner_preds (np.array): corners of the bounding box of mask\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    img_orig = cv2.imread(img_path)\n",
    "    img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = A.Compose([A.Resize(size, size)])(image = img_orig)[\"image\"]\n",
    "    \n",
    "    img = (np.transpose(img, (2, 0, 1))) / 255.0\n",
    "    img = torch.tensor(img[np.newaxis,:,:,:])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        corner_preds = model(img.to(device, dtype=torch.float32))\n",
    "        \n",
    "    corner_preds = corner_preds.detach().cpu().numpy()\n",
    "    corner_preds = np.float32(corner_preds.reshape((4,2)))\n",
    "\n",
    "    for pt in corner_preds:\n",
    "        pt[0] = pt[0]/size * 1280.0\n",
    "        pt[1] = pt[1]/size * 720.0\n",
    "\n",
    "    width, height = 1280, 720\n",
    "    target_corners = np.array([(0, 0), (width, 0), (width, height), (0, height)])\n",
    "    \n",
    "    H, _ = cv2.findHomography(corner_preds, target_corners, params=None)\n",
    "    \n",
    "    transformed_image = cv2.warpPerspective(\n",
    "        img_orig, H, (img_orig.shape[1], img_orig.shape[0]))\n",
    "    \n",
    "    return corner_preds\n",
    "\n",
    "def unet_prediction(img_path, model, preprocessing_fn, size = 320):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_path (str): path to the image\n",
    "        model (torch.nn.Module): model for segmentation\n",
    "        preprocessing_fn (callbale): data normalization function\n",
    "        size (int): size of the image to be fed to the model\n",
    "    Returns:\n",
    "        corners (np.array): corners of the bounding box of mask\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    im2 = np.array(Image.open(img_path))\n",
    "    h, w, n = im2.shape\n",
    "    true = im2.copy()\n",
    "    resize_img = A.Compose([\n",
    "            A.Resize(size, size)\n",
    "        ])\n",
    "\n",
    "    preprocessor = get_preprocessing(preprocessing_fn)\n",
    "    \n",
    "    im2 = resize_img(image = im2)['image']\n",
    "    im = im2.copy()\n",
    "    im2 = preprocessor(image = im2)['image']\n",
    "    \n",
    "    img = torch.tensor(im2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(img.unsqueeze(0).to(device, dtype = torch.float32))\n",
    "        \n",
    "    img = img.numpy()\n",
    "    out = out.sigmoid().detach().cpu().numpy()\n",
    "    temp = np.transpose(out.squeeze(0), [1,2,0]).copy()\n",
    "    \n",
    "    corners = get_contour_from_mask(np.uint8(temp*255).squeeze())\n",
    "    \n",
    "    for pt in corners:\n",
    "        pt[0] = pt[0]/size * w*1.0\n",
    "        pt[1] = pt[1]/size * h*1.0\n",
    "        \n",
    "    return corners\n",
    "\n",
    "def min_max_corner_fusion(corner_unet, corner_reg):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        corner_unet (np.array): corners of the bounding box from segmentation\n",
    "        corner_reg (np.array): corners of the bounding box from regression\n",
    "    Returns:\n",
    "        corner_min_max (np.array): corners of the bounding box from Min Max Corner Fusion Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    corner_min_max = np.zeros((4,2))\n",
    "        \n",
    "    corner_min_max[0][0] = min(corner_unet[0][0], corner_reg[0][0])\n",
    "    corner_min_max[0][1] = min(corner_unet[0][1], corner_reg[0][1])\n",
    "    corner_min_max[1][0] = max(corner_unet[1][0], corner_reg[1][0])\n",
    "    corner_min_max[1][1] = min(corner_unet[1][1], corner_reg[1][1])\n",
    "    corner_min_max[2][0] = max(corner_unet[2][0], corner_reg[2][0])\n",
    "    corner_min_max[2][1] = max(corner_unet[2][1], corner_reg[2][1])\n",
    "    corner_min_max[3][0] = min(corner_unet[3][0], corner_reg[3][0])\n",
    "    corner_min_max[3][1] = max(corner_unet[3][1], corner_reg[3][1])\n",
    "    \n",
    "    return corner_min_max\n",
    "\n",
    "def screen_extraction(img_path, model_unet, model_reg, preprocessing_unet, mode = 'accurate'):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_path (str): path to the image\n",
    "        model_unet (torch.nn.Module): unet model\n",
    "        model_reg (torch.nn.Module): corner regression model\n",
    "        preprocessing_unet (callbale): data normalization function of unet\n",
    "        mode (str): mode of the algorithm\n",
    "    Returns:\n",
    "        transformed_image (np.array): transformed image\n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if mode == 'accurate':\n",
    "        \n",
    "        corner_unet = unet_prediction(img_path, model_unet, preprocessing_unet)\n",
    "        corner_reg = corner_regression(img_path, model_reg)\n",
    "        corner_preds = min_max_corner_fusion(corner_unet, corner_reg)\n",
    "        \n",
    "    else:\n",
    "        corner_preds = corner_regression(img_path, model_reg)\n",
    "    \n",
    "    for pt in corner_preds:\n",
    "        pt[0] = pt[0]/img.shape[1] * 1280.0\n",
    "        pt[1] = pt[1]/img.shape[0] * 720.0\n",
    "        \n",
    "        \n",
    "    width, height = 1280, 720\n",
    "    target_corners = np.array([(0, 0), (width, 0), (width, height), (0, height)])\n",
    "\n",
    "    # Get matrix H that maps source_corners to target_corners\n",
    "    H, _ = cv2.findHomography(corner_preds, target_corners, params=None)\n",
    "\n",
    "    # Apply matrix H to source image.\n",
    "    transformed_image = cv2.warpPerspective(\n",
    "        img, H, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "#     plt.imshow(transformed_image)\n",
    "    \n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu5njhI-105W"
   },
   "source": [
    "# Number Detection + OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnqX6USFIM8H"
   },
   "source": [
    "After extracting the screens, our next stage in the pipeline is to find all the numbers on the screen and return the appropriate bounding boxes across them. This task is achieved with the help of two separate models: \n",
    "  \n",
    "  - **YOLOv5** \n",
    "  - **PaddleOCR**\n",
    "      \n",
    "      * YOLOv5: \n",
    "              \n",
    "          We trained YOLOv5 model for the detection of numbers on a screen. We assigned a “number” class to all given bounding box and then fine-tuned YOLO for the same. The details of training and hyperparameters are as follows:\n",
    "\n",
    "          * Model: YOLOv5\n",
    "          * Size: M\n",
    "          * Epochs: 25\n",
    "          * Learning Rate: 0.01 \n",
    "          * Mean Inference time: 650 ms\n",
    "          * Average Precision: 0.77\n",
    "\n",
    "      * PaddleOCR:\n",
    "              \n",
    "         PaddleOCR is an optical character detection and recognition model implemented using PaddlePaddle (PArallel Distributed Deep LEarning) framework. In our pipeline, we have used Pre-trained PaddleOCR to detect numbers from the extracted screen. The details of the hyperparameters are as follows:\n",
    "          * Recognition Algorithm: CRNN\n",
    "          * Detection Algorithm: DB\n",
    "          * Mean Inference Time: 2.5 s\n",
    "\n",
    "    * Weighted Box Fusion of PaddleOCR and YOLO:\n",
    "              \n",
    "         Fine-tuned YOLOv5 is showing promising results on layouts it was trained on but a relatively low accuracy on unseen layouts. In any case, it was not giving any noise in the prediction. Pre-trained PaddleOCR captures boxes of all numbers on the screens, but being a text recognition model, it also predicts few noise in the screens, which is not required. Hence, we ensemble the predictions of both the models, using Weighted Box Fusion algorithm, taking the good points of both algorithms, thereby resulting in much more robust predictions.\n",
    "      * Weighted Box Fusion algorithm utilizes confidence scores of all proposed bounding boxes to construct the averaged boxes. \n",
    "\n",
    "       * Mean Inference Time - 3.3 s\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXUZ5nw8IRrg"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwJ5dozIIVls"
   },
   "source": [
    "\n",
    "After getting all the bounding boxes around the numbers in the image, we use Optical Character Recognition to get the numbers written in the images. We implement this task using the ParSeq (Permuted AutoRegressive SEQuence) model. We are using the pre-trained model of ParSeq, and the details are as follows:\n",
    "\n",
    "  * **Model: ParSeq**\n",
    "  * **Accuracy: 0.95**\n",
    "  * **Mean Inference Time: 1 s**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (1.6.5)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (4.64.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (2022.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.3.2)\n",
      "Collecting protobuf<=3.20.1\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-macosx_10_9_x86_64.whl (962 kB)\n",
      "\u001b[K     |████████████████████████████████| 962 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.21.6)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (2.9.1)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.11.4)\n",
      "Requirement already satisfied: torch>=1.8.* in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (4.5.0)\n",
      "Requirement already satisfied: requests in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (61.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.33.0)\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-macosx_10_9_x86_64.whl (980 kB)\n",
      "\u001b[K     |████████████████████████████████| 980 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch_lightning) (2.1.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visualdl 2.5.1 requires protobuf>=3.20.0, but you have protobuf 3.19.6 which is incompatible.\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\n",
      "Successfully installed protobuf-3.19.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning==1.6.5\n",
      "  Using cached pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (4.1.1)\n",
      "Requirement already satisfied: torch>=1.8.* in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (1.12.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (1.21.6)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (2.9.1)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (0.11.4)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (2022.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (21.3)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (3.19.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.6.5) (6.0)\n",
      "Requirement already satisfied: requests in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning==1.6.5) (2.4.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (1.2.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (2.0.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (61.2.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.5) (1.33.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (2022.12.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.6.5) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (1.6.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aryanlath/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.5) (1.2.0)\n",
      "Installing collected packages: pyDeprecate, pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.0.1.post0\n",
      "    Uninstalling pytorch-lightning-2.0.1.post0:\n",
      "      Successfully uninstalled pytorch-lightning-2.0.1.post0\n",
      "Successfully installed pyDeprecate-0.3.2 pytorch-lightning-1.6.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch_lightning==1.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlRZLZmW13pg",
    "outputId": "da9d8f42-efc4-4c1d-a25e-255e6f254be9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PaddleOCR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\n\u001b[1;32m      3\u001b[0m yolo_model_det \u001b[38;5;241m=\u001b[39m yolov5\u001b[38;5;241m.\u001b[39mload(root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/final_yolo_weights.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m paddle_ocr_det_acc \u001b[38;5;241m=\u001b[39m \u001b[43mPaddleOCR\u001b[49m(cpu_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, rec_batch_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, rec_algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCRNN\u001b[39m\u001b[38;5;124m'\u001b[39m, rec_image_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model_ocr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaudm/parseq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparseq\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m yolo_fast \u001b[38;5;241m=\u001b[39m yolov5\u001b[38;5;241m.\u001b[39mload(root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo_on_6_fast.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PaddleOCR' is not defined"
     ]
    }
   ],
   "source": [
    "## Loading models for detection\n",
    "import pytorch_lightning\n",
    "yolo_model_det = yolov5.load(root + '/final_yolo_weights.pt')\n",
    "paddle_ocr_det_acc = PaddleOCR(cpu_threads=1, rec_batch_num=2, rec_algorithm='CRNN', rec_image_inverse=False)\n",
    "model_ocr = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval().to(device)\n",
    "yolo_fast = yolov5.load(root + 'yolo_on_6_fast.pt')\n",
    "paddle_fast = PaddleOCR(use_angle_cls=False, lang='en', ocr_version = 'PP-OCR', structure_version = 'PP-Structure', \n",
    "                rec_algorithm = 'CRNN', max_text_length = 200, use_space_char = False, lan = 'en', det = False,\n",
    "                cpu_threads = 12, cls = False,use_gpu=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "WBNM8FCSGdba"
   },
   "outputs": [],
   "source": [
    "yolo_model_det.eval();\n",
    "model_ocr.eval();\n",
    "yolo_fast.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "6rSob5U03g2D"
   },
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "def return_fast_output(yolo_model, img):\n",
    "\n",
    "    image = img.copy()\n",
    "\n",
    "    results_yolo = yolo_model(img)\n",
    "\n",
    "    try:\n",
    "        boxes = results_yolo.pred[0][:, :4].tolist()\n",
    "        scores = results_yolo.pred[0][:, 4].tolist()\n",
    "        labels = results_yolo.pred[0][:, 5].tolist()\n",
    "    except:\n",
    "        boxes = []\n",
    "        scores_yolo = []\n",
    "        labels_yolo = []\n",
    "    \n",
    "    dic = {}\n",
    "    for each in labels:\n",
    "        if each not in dic.keys():\n",
    "            dic[each] = (0,[])\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        score , box = dic[labels[i]]\n",
    "        if score < scores[i]:\n",
    "            dic[labels[i]] = (scores[i], boxes[i])\n",
    "    \n",
    "    # print(dic)\n",
    "    return dic\n",
    "\n",
    "def recognize_fast(image,dic,rec):\n",
    "\n",
    "    vitals = {}\n",
    "    labels = {0.0: 'DBP' , 1.0:'HR' , 2.0:'MAP', 3.0:'RR' ,4.0:'SBP' ,5.0:'SPO2' }\n",
    "    for each in dic.keys():\n",
    "        score, box = dic[each]\n",
    "        xmin = int(box[0])\n",
    "        xmax = int(box[2])\n",
    "        ymin = int(box[1])\n",
    "        ymax = int(box[3])\n",
    "        img = image[ymin:ymax,xmin:xmax]\n",
    "        text = rec.ocr(img,cls = False,det = False)[0][0][0]\n",
    "        text = text.replace('(','').replace(')','').replace('/','').replace('-','').replace('*','')\n",
    "        if text.isdigit():\n",
    "            vitals[labels[each]] = text\n",
    "\n",
    "    return vitals\n",
    "\n",
    "def return_output(yolo_model, paddle_ocr, img):\n",
    "  image = img.copy()\n",
    "  results_yolo = yolo_model(img)\n",
    "  try:\n",
    "    boxes = results_yolo.pred[0][:, :4].tolist()\n",
    "    scores_yolo = results_yolo.pred[0][:, 4].tolist()\n",
    "    labels_yolo = results_yolo.pred[0][:, 5].tolist()\n",
    "  except:\n",
    "    boxes = []\n",
    "    scores_yolo = []\n",
    "    labels_yolo = []\n",
    "  boxes_yolo = []\n",
    "  for box in boxes:\n",
    "    boxes_yolo.append([box[0]/1280, box[1]/720, box[2]/1280, box[3]/720])\n",
    "  # results_paddle = paddle_ocr.ocr(img, cls=False, rec = True)\n",
    "  # final_boxes_paddle = []\n",
    "  # final_confidence_paddle = []\n",
    "  # final_labels_paddle = [] \n",
    "  # for i in range(len(results_paddle[0])):\n",
    "  #   xmin = results_paddle[0][i][0][0][0]/1280.\n",
    "  #   ymin = results_paddle[0][i][0][0][1]/720.\n",
    "  #   xmax = results_paddle[0][i][0][2][0]/1280.\n",
    "  #   ymax = results_paddle[0][i][0][2][1]/720.\n",
    "  #   temp = [xmin, ymin, xmax, ymax]\n",
    "\n",
    "  #   # area\n",
    "  #   if (xmax*1280 - xmin*1280)*(ymax*720 - ymin*720) < 1000:\n",
    "  #     continue\n",
    "    \n",
    "  #   if (xmax*1280 - xmin*1280) < 120:\n",
    "  #     continue\n",
    "  #   if (xmax*1280 - xmin*1280) > 300:\n",
    "  #     continue\n",
    "  #   final_boxes_paddle.append(temp)\n",
    "  #   final_confidence_paddle.append(results_paddle[0][i][1][1])\n",
    "  # for i in range(len(final_confidence_paddle)):\n",
    "  #   final_labels_paddle.append(0)\n",
    "  result_box = boxes_yolo\n",
    "  result_conf = scores_yolo\n",
    "  result_label = labels_yolo\n",
    "  return result_box, result_conf, result_label, img\n",
    "  \n",
    "################################################################################\n",
    "\n",
    "def wbf_ensemble(boxes_list, scores_list, labels_list, image):\n",
    "  weights = [2, 1]\n",
    "  iou_thr = 0.6\n",
    "  skip_box_thr = 0.01\n",
    "  sigma = 0.1\n",
    "  boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "  return boxes, scores\n",
    "\n",
    "def recognize(image,boxes,scores):\n",
    "    imgs = []\n",
    "    for box in boxes:\n",
    "        xmin = int(box[0]*1280)\n",
    "        ymin = int(box[1]*720)\n",
    "        xmax = int(box[2]*1280)\n",
    "        ymax = int(box[3]*720)\n",
    "        img = image[ymin:ymax,xmin:xmax]\n",
    "        imgs.append(img)\n",
    "    \n",
    "    procs = [preproc_image(img) for img in imgs]\n",
    "    preds = model_ocr(torch.cat(procs, dim=0))\n",
    "    labels = inference_pred(preds)\n",
    "    \n",
    "    return labels,image,boxes,scores\n",
    "\n",
    "def preproc_image(img):\n",
    "    img = Image.fromarray(img).convert('RGB')\n",
    "    transform = T.Compose([\n",
    "            T.Resize((32, 128)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "    img = transform(img)\n",
    "    return img.unsqueeze(0)\n",
    "\n",
    "def inference_pred(pred):\n",
    "    pred = pred.softmax(-1)\n",
    "    label, _ = model_ocr.tokenizer.decode(pred)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "eK3rLVAc6qGQ"
   },
   "outputs": [],
   "source": [
    "def check_string(string):\n",
    "  if(('(' in string or ')' in string) and '/' in string):\n",
    "      return False\n",
    "  elif (string.count('(') > 1 or string.count('/') > 1 or string.count(')') > 1):\n",
    "      return False\n",
    "  string = string.replace('(', '').replace('/', '').replace(')', '')\n",
    "  pattern = r'^[\\d]+$'\n",
    "  return re.match(pattern, string) != None\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def image_dict(text , boxes , scores, image):\n",
    "  c = 0\n",
    "  text_l = []\n",
    "  boxes_l = []\n",
    "  scores_l = []\n",
    "\n",
    "  for i, t in enumerate(text):\n",
    "    if check_string(t):\n",
    "      text_l.append(t)\n",
    "      boxes_l.append(boxes[i])\n",
    "      scores_l.append(scores[i])\n",
    "\n",
    "  boxes_l, scores_l = np.array(boxes_l), np.array(scores_l)\n",
    "  nums = np.array([float(txt.replace('(', '').replace('/', '').replace(')', '')) for txt in text_l])\n",
    "  try:\n",
    "    ind = np.argsort(scores_l)[-6:]\n",
    "    scores_l = scores_l[ind]\n",
    "    text_l = [text_l[x] for x in ind]\n",
    "    boxes_l = boxes_l[ind]\n",
    "    nums = nums[ind]\n",
    "  except:\n",
    "    pass\n",
    "  boxes_dic = []\n",
    "  for i,num in enumerate(text_l):\n",
    "    bbxi = boxes_l[i]\n",
    "    nm = nums[i]\n",
    "    text_data = np.array([0.0, 0.0, 0.0])\n",
    "    if '/' in num:\n",
    "      text_data[0] = 1.0\n",
    "    if '(' in num:\n",
    "      text_data[1] = 1.0\n",
    "    if ')' in num:\n",
    "      text_data[2] = 1.0\n",
    "    boxes_dic.append({'bbox': bbxi, 'num': nm, 'text_data': text_data})\n",
    "    \n",
    "  return {'image': image, 'val_vec': nums.tolist(), 'boxes': boxes_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "-fTXJ6NQ32aB"
   },
   "outputs": [],
   "source": [
    "def number_detection(img, mode = 'accurate'):\n",
    "\n",
    "  if mode == 'accurate':\n",
    "    \n",
    "    boxes, scores, result_label, img = return_output(yolo_model_det, paddle_ocr_det_acc, img)\n",
    "    # result_box, result_conf, result_label, img = return_output(yolo_model_det, paddle_ocr_det_acc, img)\n",
    "\n",
    "    # boxes, scores, img = wbf_ensemble(result_box, result_conf, result_label, img)\n",
    "\n",
    "    text , img, boxes , scores = recognize(img, boxes,scores)\n",
    "\n",
    "    number_dict = image_dict(text , boxes , scores, img)\n",
    "\n",
    "  else:\n",
    "\n",
    "    temp = return_fast_output(yolo_fast, img)\n",
    "\n",
    "    number_dict = recognize_fast(img, temp, paddle_fast)\n",
    "\n",
    "  return number_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An-1KX5j7Ufh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yiuk34Bd7gE0"
   },
   "source": [
    "# Number Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_TnvdPvIbvz"
   },
   "source": [
    "This task consists of classifying all the numbers detected by OCR into their respective classes. To implement this task, we have made **CRABBNet (Custom Recognition Assisted Bounding Box classification Network)**.  \n",
    "  CRABBNet takes 3 inputs, a 4-channel Image, all numbers on the screen, and the target number which needs to be classified. \n",
    "\n",
    "  The working of CRABBNet is explained below:\n",
    "  \n",
    "  * The screen-extracted image (of 3 RGB channels) on which the prediction has to be made is concatenated with a 1-channel mask of the bounding box of the target number to produce a 4-channel input for the network.\n",
    "\n",
    "  * This 4-channel input is passed to a ResNeXt - 50 model, which produces a 2048-length feature vector of the image. \n",
    "A 14-length vector is created, which contains information about all the numbers on the screen.\n",
    "\n",
    "  * The 10 numbers in the vector denote the 10 numbers which are present on the screen. If less numbers are present, the remaining values are filled with 0\n",
    "The next 3 numbers are binary values, indicating presence of ‘/’, ‘(‘ and ‘)’  in our target number which is to be classified. This is because, many a times in prediction of DBP and SBP, ‘/’ is seen in the image, while in prediction of MAP, ‘(‘ and ‘)’ are found.\n",
    "\n",
    "* The last number of the vector is the target number itself.\n",
    "\n",
    "* This 14-length vector is then passed to a linear layer, which converts this into 6-length vector. \n",
    "* This 6-length vector, is concatenated with the previous 2048-length feature vector, thereby resulting in 2054-length vector.\n",
    "* This 2054-length vector is again concatenated with the target number, to give more weight to the target number, informing model to focus on predicting the target number.\n",
    "* Hence, after this step, we would result in 2055-length vector.\n",
    "This 2055-length vector is then passed again to a linear layer, with softmax activation, which gives probability of 6 classes among which the number would be present.\n",
    "\n",
    "    * **Training Methodology**:\n",
    "        * **Model : Resnext50_32x4d**\n",
    "        * **Epochs : 15**\n",
    "        * **Learning_rate : 0.001**\n",
    "        * **Loss : CrossEntropy** \n",
    "        * **Accuracy :    77% (Out-of-fold Layout Validation) and 98% (Single Layout training)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7lrNh5MIgW8"
   },
   "source": [
    "#### **Custom Logit-Decoder for Multi-label Matching**:\n",
    "\n",
    "Using the above model we generate 6-class predictions for each bounding box on the screen. To combine predictions from multiple bounding boxes in multiple classes while ensuring single prediction for each vital, we use a customized probability decoder.\n",
    "\n",
    "\n",
    "We use variance measure across each class prediction to get the most confident class and decide the box prediction for that class. This step is repeated on each class in the order of decreasing variance while simultaneously already selected bounding boxes to ensure a one-to-one mapping of bounding boxes and classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8_Z7e3Um7iPH"
   },
   "outputs": [],
   "source": [
    "class CRABBNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('resnext50_32x4d', pretrained=True, num_classes=0, in_chans=4)\n",
    "        self.ll = nn.Linear(2048+7,6)\n",
    "        self.fl = nn.Linear(14,6)\n",
    "    \n",
    "    def forward(self, img, text_data, num, val_vec):\n",
    "        x = self.model(img)\n",
    "        x = torch.cat([x, num.view(-1, 1), self.fl(torch.cat([num.view(-1, 1), text_data.view(-1, 3), val_vec.view(-1, 10)], dim=1))], dim=1)\n",
    "        x = self.ll(x)\n",
    "        x = x.view(-1, 6)\n",
    "#         x = F.softmax(x, dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nFLVM4tC7wOa"
   },
   "outputs": [],
   "source": [
    "model_crabb = CRABBNET()\n",
    "model_crabb = model_crabb.to(device)\n",
    "model_crabb.load_state_dict(torch.load(root + '/weights/crabbnet.pt', map_location = device))\n",
    "model_crabb.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "a9-4PZun722n"
   },
   "outputs": [],
   "source": [
    "class InferDataset(Dataset):\n",
    "    def __init__(self, img_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dict = img_dict\n",
    "                          \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_dict['image']\n",
    "        val_vec = np.array(self.img_dict['val_vec'])\n",
    "        val_vec.resize(10,)\n",
    "        np.random.shuffle(val_vec)\n",
    "        \n",
    "        boxes = self.img_dict['boxes'][idx]  \n",
    "        bbox = boxes['bbox']\n",
    "        \n",
    "        xmin = int(bbox[0]*1280)\n",
    "        ymin = int(bbox[1]*720)\n",
    "        xmax = int(bbox[2]*1280)\n",
    "        ymax = int(bbox[3]*720)\n",
    "\n",
    "        mask = np.zeros((img.shape[0], img.shape[1], 1))\n",
    "        mask = cv2.rectangle((mask), (xmin, ymin), (xmax, ymax), 255, -1)\n",
    "\n",
    "        # plt.imshow(mask[:,:,0])\n",
    "        # plt.show()\n",
    "        \n",
    "        num = boxes['num']\n",
    "        text_data = boxes['text_data']\n",
    "\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        mask = cv2.resize(mask, (224,224))[:,:,None]\n",
    "                \n",
    "        arr4 = np.concatenate([img, mask], axis = 2)\n",
    "                   \n",
    "        arr4 = (np.transpose(arr4, (2, 0, 1))) / 255.0\n",
    "        arr4 = torch.tensor(arr4)\n",
    "        val_vec = torch.tensor(val_vec)\n",
    "                \n",
    "        return arr4, torch.tensor(num), torch.tensor(text_data), val_vec\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_dict['boxes'])\n",
    "\n",
    "########################\n",
    "\n",
    "def pred_organizing(pred_mat, nums):\n",
    "    num_boxes = nums.shape[0]\n",
    "    box_store = set(range(num_boxes))\n",
    "    mat_variance = torch.var(pred_mat, dim=0)\n",
    "    argmax_dim0 = torch.argmax(pred_mat, dim=0)\n",
    "    sorted_, indices = torch.sort(mat_variance, descending=True)\n",
    "    \n",
    "    res_dict = {'HR':None, 'RR':None, 'SPO2':None, 'SBP':None, 'DBP':None, 'MAP':None}\n",
    "    label_cols = ['HR', 'RR', 'SPO2', 'SBP', 'DBP', 'MAP']\n",
    "    \n",
    "    for ind in indices:\n",
    "        \n",
    "        argmax_dim0 = torch.argmax(pred_mat, dim=0)\n",
    "        box_ind = argmax_dim0[ind].item()\n",
    "\n",
    "        if box_ind not in box_store:\n",
    "            present = list(box_store)\n",
    "            if len(present) == 0:\n",
    "                continue\n",
    "            \n",
    "        res_dict[label_cols[ind]] = nums[box_ind]\n",
    "        pred_mat[box_ind, :] = -100000\n",
    "    \n",
    "        if torch.unique(pred_mat).shape[0] == 1:\n",
    "            break\n",
    "    \n",
    "    return res_dict\n",
    "\n",
    "############################\n",
    "\n",
    "def final_inference(img_dict):\n",
    "    test_img = InferDataset(img_dict)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "            dataset=test_img,\n",
    "            batch_size = len(img_dict['boxes']),\n",
    "            num_workers = 0,\n",
    "    )\n",
    "\n",
    "\n",
    "    imgs, nums, text_data, val_vec = next(iter(test_loader))\n",
    "\n",
    "    imgs = imgs.float()\n",
    "    nums = nums.float()\n",
    "    text_data = text_data.float()\n",
    "    val_vec = val_vec.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        label_preds = model_crabb(imgs, text_data, nums, val_vec)\n",
    "\n",
    "    nums = nums.numpy()\n",
    "\n",
    "    yerr_dict = pred_organizing(label_preds, nums)\n",
    "    yerr_dict = {k:v for k,v in yerr_dict.items() if v is not None}\n",
    "\n",
    "    return yerr_dict\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MYi-Xjk9Xf4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1gLNFZZ9btR"
   },
   "source": [
    "# Final Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "dY_ctVF89eY8"
   },
   "outputs": [],
   "source": [
    "def vital_extraction(img_path, mode = 'accurate'):\n",
    "\n",
    "  if mode == 'accurate':\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    transformed_image = screen_extraction(img_path, model_unet, model_reg, preprocessing_unet, mode)\n",
    "\n",
    "    detection_dict = number_detection(transformed_image, mode)\n",
    "\n",
    "    output_dict = final_inference(detection_dict)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "  else:\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    transformed_image = screen_extraction(img_path, model_unet, model_reg, preprocessing_unet, mode)\n",
    "\n",
    "    detection_dict = number_detection(transformed_image, mode)\n",
    "\n",
    "    return detection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAiTOyJiSGS4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
